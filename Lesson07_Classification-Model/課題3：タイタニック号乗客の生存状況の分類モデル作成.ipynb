{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR57YiF4JLsT"
      },
      "source": [
        "# 課題3：タイタニック号乗客の生存状況の分類モデル作成\n",
        "\n",
        "本課題では、`titanic` というデータセットを使います。これは、1912年に発生したタイタニック号の沈没事故における乗客の生存状況に関するデータセットです。元々は、[Encyclopedia Titanica](https://www.encyclopedia-titanica.org/)で掲載されたデータと言われており、このデータセットを組み込んだPythonのライブラリも複数あります。\n",
        "\n",
        "今回は、`seaborn` のライブラリに組み込まれた `titanic` のデータセットを使います。各セルに入っているコメントの下に、実行するコードを記入してください。わからない場合は、ここまでのレッスン内容や各種ライブラリの公式ドキュメントを参照しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk0nknCgLySU"
      },
      "source": [
        "## 1. 必要なライブラリのimport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EInYuSc2JHNX"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリのimport（変更しないでください）\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# DataFrameですべての列を表示する設定（変更しないでください）\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVBdaqf5L_tJ"
      },
      "source": [
        "## 2. データの読み込み\n",
        "\n",
        "seabornに添付のデータセットから「titanic」を読み込み、内容を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz-PggfGL_8x"
      },
      "outputs": [],
      "source": [
        "# seabornからtitanicのデータセットを読み込む（変更しないでください）\n",
        "dataset = sns.load_dataset(\"titanic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd0XsSMNNPx"
      },
      "source": [
        "`sns.load_dataset()` で読み込んだデータは、pandasのDataFrameになっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJdbeGwlMGbB"
      },
      "outputs": [],
      "source": [
        "# datasetの先頭5件を確認\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74U0U26aMQnN"
      },
      "source": [
        "### 使用する列の指定\n",
        "\n",
        "今回は `survived, pclass, sex, age, sibsp, parch, fare, embarked` の列を使用します。\n",
        "\n",
        "#### 参考:各列の説明\n",
        "\n",
        "- `survived`: 生存区分（0:死亡, 1:生存）\n",
        "- `pclass`: チケットクラス\n",
        "- `sex`: 性別（male:男性, female:女性）\n",
        "- `age`: 年齢\n",
        "- `sibsp`: 同乗している兄弟や配偶者の数\n",
        "- `parch`: 同乗している親や子供の数\n",
        "- `fare`: 料金\n",
        "- `embarked`: 乗船した港（頭文字）\n",
        "- `class`: 客室クラス\n",
        "- `who`: 性別（man:男性, woman:女性）\n",
        "- `adult_male`: 成人男性ならTrue\n",
        "- `deck`: 事故の際にどのデッキにいたか\n",
        "- `embark_town`: 乗船した港名\n",
        "- `alive`: 生存区分（no:死亡, yes:生存）\n",
        "- `alone`: 1人で乗船したか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVlsbngKMPG3"
      },
      "outputs": [],
      "source": [
        "# datasetから「survived, pclass, sex, age, sibsp, parch, fare, embarked」の列を取得して\n",
        "# datasetに代入（上書き）する\n",
        "dataset = dataset[[\"survived\", \"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTIs48AFNXoO"
      },
      "outputs": [],
      "source": [
        "# 改めてdatasetの先頭5件を表示\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDE9b6RCorEp"
      },
      "source": [
        "## 3. データの前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50vIEWrmosTq"
      },
      "source": [
        "### 要約統計量の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQFcOcGlNkrp"
      },
      "outputs": [],
      "source": [
        "# 要約統計量を表示\n",
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb-ISfCUoycZ"
      },
      "source": [
        "### 欠損値の確認と補完"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86RHxbsmovmM"
      },
      "outputs": [],
      "source": [
        "# 各列の欠損値の数を確認\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jam6Dly8pA2Y"
      },
      "source": [
        "ageの欠損値は平均値で補完します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42JjLjJPo4Kj"
      },
      "outputs": [],
      "source": [
        "# ageの欠損値を、ageの平均値で補完する\n",
        "dataset[\"age\"] = dataset[\"age\"].fillna(dataset[\"age\"].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHDf1gcHpIJi"
      },
      "source": [
        "embarkedの欠損値は、もっとも乗船者数の多い港で補完します。\n",
        "\n",
        "その方法はいくつかありますが、ここではその1つとして、DataFrameの特定の1列（Series）が持つ `value_counts()` メソッドを紹介します。このメソッドを実行すると、その列が持つ値ごとのデータ数がわかります。\n",
        "\n",
        "参考：[pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOe3rABEpacX"
      },
      "outputs": [],
      "source": [
        "# 乗船者数の多い港を value_counts メソッドで確認\n",
        "dataset[\"embarked\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS73opx9qXDe"
      },
      "source": [
        "`values_count()` の結果を見て、もっとも乗船者数の多い港の文字で欠損値を埋めるようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faq_eB30qfHF"
      },
      "outputs": [],
      "source": [
        "# 最頻値（もっとも乗船者数が多い港）を取得\n",
        "most_common_port = dataset[\"embarked\"].mode()[0]\n",
        "\n",
        "# 欠損値を最頻値で補完\n",
        "dataset[\"embarked\"] = dataset[\"embarked\"].fillna(most_common_port)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjiIM6odqka_"
      },
      "source": [
        "上記の処理により、欠損値がなくなったかを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F6FqAlwqoYC"
      },
      "outputs": [],
      "source": [
        "# 欠損値の数を確認し、補完後の欠損値が0であることを確認\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vo28PuRrkWg"
      },
      "source": [
        "### ダミー変数への変換\n",
        "\n",
        "sexとembarkedをダミー変数に変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5YCZdd1qtxw"
      },
      "outputs": [],
      "source": [
        "# datasetのsexとembarkedをダミー変数に変換してdataset2に代入する\n",
        "dataset2 = pd.get_dummies(dataset, columns=[\"sex\", \"embarked\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuWwmcTwrr52"
      },
      "outputs": [],
      "source": [
        "# dataset2のデータの最初の5行を表示\n",
        "dataset2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcvc581SrvcB"
      },
      "source": [
        "## 4. 目的変数と説明変数の選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2An5kUmCr1Wa"
      },
      "source": [
        "ここでは、以下の列を使用します。\n",
        "\n",
        "- 目的変数: `survived`\n",
        "- 説明変数: それ以外\n",
        "\n",
        "dataset2より目的変数と説明変数に該当する列を取得してnumpy配列に変換し、変数YとXに格納します。列の除外には、DataFrameの `drop` を使います。`データフレーム.drop(columns=除外したい列名)` です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VTkw80mrsKy"
      },
      "outputs": [],
      "source": [
        "# Y:目的変数に該当する列\n",
        "Y = dataset2[\"survived\"].values\n",
        "\n",
        "# X:説明変数に該当する列。dataset2からsurvivedを除外\n",
        "X = dataset2.drop(columns=[\"survived\"]).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvOHEbKnsa_I"
      },
      "outputs": [],
      "source": [
        "# YとXの形状を確認\n",
        "Y.shape, X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHO8aytvsmhE"
      },
      "source": [
        "## 5. データの分割\n",
        "\n",
        "この課題ではホールドアウト法でデータを分割します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfiTpx0NscZY"
      },
      "outputs": [],
      "source": [
        "# X と Y を 機械学習用データとテストデータに7:3に分ける(X_train, X_test, Y_train, Y_test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCPzHIXYsuHW"
      },
      "outputs": [],
      "source": [
        "# 機械学習用データを、学習データと検証データに7:3に分ける(X_train, X_valid, Y_train, Y_valid)\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
        "    X_train, Y_train, test_size=0.3, random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsBDH_A2swJl"
      },
      "outputs": [],
      "source": [
        "# 形状を確認:X_train, X_valid, X_test, Y_train, Y_valid, Y_test\n",
        "X_train.shape, X_valid.shape, X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmYFv2ojs4pS"
      },
      "source": [
        "## 6. モデルの選択\n",
        "\n",
        "ロジスティック回帰と決定木、ランダムフォレスト、SVMの4つのモデルを作成して比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xvP1N7ksyIE"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリの追加import（変更しないでください）\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3URsWYtKQf"
      },
      "source": [
        "モデルの評価（性能の比較）には、F1値を使ってください。以下には1つだけセルを用意していますが、モデルを4つ作って比較する処理のためにセルを増やしてもかまいません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSMqJrrCtCRN"
      },
      "outputs": [],
      "source": [
        "# 4つのモデルを作成し、それぞれのF1値を出力する\n",
        "# モデルの定義\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
        "    \"SVC\": SVC()\n",
        "}\n",
        "\n",
        "# 各モデルのF1スコアを計算\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, Y_train)               # 学習\n",
        "    Y_pred = model.predict(X_valid)           # 検証データで予測\n",
        "    f1 = f1_score(Y_valid, Y_pred)            # F1値の算出\n",
        "    print(f\"{name}: F1 Score = {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSsGuxitukuL"
      },
      "source": [
        "## 7. パラメータのチューニング\n",
        "\n",
        "GridSearchCVを使い、性能の良かったランダムフォレストのパラメータのチューニングを行ないます。パラメータの候補については、レッスン本編を参考にしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGC4Ud3YujSN"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリの追加import（変更しないでください）\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r88C6Ix7uqoq"
      },
      "outputs": [],
      "source": [
        "# 性能の良かったモデルを作成\n",
        "best_model = RandomForestClassifier(random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezbl7chou4SG"
      },
      "outputs": [],
      "source": [
        "# パラメータの指定\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],     # 決定木の本数\n",
        "    \"max_depth\": [None, 5, 10, 20],     # 木の深さ（None=制限なし）\n",
        "    \"min_samples_split\": [2, 5, 10],    # ノード分割に必要なサンプル数\n",
        "    \"min_samples_leaf\": [1, 2, 4]       # 葉ノードの最小サンプル数\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX2iEosnu98q"
      },
      "outputs": [],
      "source": [
        "# グリッドサーチのオブジェクトを作成\n",
        "grid = GridSearchCV(\n",
        "    estimator=best_model,        # チューニングするモデル\n",
        "    param_grid=param_grid,       # 探索するパラメータ\n",
        "    cv=5,                        # 5分割交差検証\n",
        "    scoring=\"f1\",                # F1スコアで評価\n",
        "    n_jobs=-1                    # 並列実行で高速化\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm7WuLwKvFm-"
      },
      "outputs": [],
      "source": [
        "# データの分割:機械学習用データを学習と検証に分けるのはクロスバリデーションで行ってくれる\n",
        "# （Xg_train, Xg_test, Yg_train, Yg_test）\n",
        "Xg_train, Xg_test, Yg_train, Yg_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJdSlEPBvMpG"
      },
      "outputs": [],
      "source": [
        "# グリッドサーチを実行する\n",
        "grid.fit(Xg_train, Yg_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUL9ispJvSYN"
      },
      "outputs": [],
      "source": [
        "# 最適なパラメータを表示\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlvZoIKTvqgM"
      },
      "source": [
        "ここで得たパラメータをもとに、モデルを再度作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doYjPMUovZ77"
      },
      "outputs": [],
      "source": [
        "# 最適なパラメータによるモデルの作成\n",
        "best_params = grid.best_params_\n",
        "best_model = RandomForestClassifier(**best_params, random_state=0)\n",
        "# モデルの学習\n",
        "best_model.fit(Xg_train, Yg_train)\n",
        "\n",
        "# モデルの予測\n",
        "Yg_pred = best_model.predict(Xg_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yULwXTP0wABh"
      },
      "outputs": [],
      "source": [
        "# F1値の出力\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_value = f1_score(Yg_test, Yg_pred)\n",
        "print(\"F1 Score:\", f1_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyjl2H4PwEt9"
      },
      "source": [
        "## 8. テストデータによる汎化性能の確認\n",
        "\n",
        "最後にテストデータでモデルの汎化性能を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G194ua48wFIh"
      },
      "outputs": [],
      "source": [
        "# テストデータを使って予測を行いF1値を算出\n",
        "# テストデータで予測\n",
        "Yg_pred_test = best_model.predict(Xg_test)\n",
        "\n",
        "# F1値の算出\n",
        "from sklearn.metrics import f1_score\n",
        "f1_test = f1_score(Yg_test, Yg_pred_test)\n",
        "\n",
        "print(\"F1 Score (Test Data):\", f1_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
